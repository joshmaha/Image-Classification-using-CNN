{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshmaha/Image-Classification-using-CNN/blob/main/cifar10_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "YDsocSbsxEnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### import needed libraries"
      ],
      "metadata": {
        "id": "s_IbAxcN50XU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import needed libraries\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "OlLgvTWMxYIX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train models"
      ],
      "metadata": {
        "id": "HsEQGKfO5w87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure we use gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(\"Using device:\", device)\n",
        "\n",
        "# make 2 different networks\n",
        "net1 = Net().to(device)\n",
        "net2 = Net().to(device)\n",
        "\n",
        "\n",
        "# shuffle data differently by changing shuffle orders\n",
        "order1 = np.random.randint(0, 100000) # generate random number for firs network\n",
        "order2 = np.random.randint(0, 100000)\n",
        "\n",
        "rand_shuffle_net1 = torch.Generator().manual_seed(order1)  # manual_seed sets the starting point for the randomness at seed1\n",
        "rand_shuffle_net2 = torch.Generator().manual_seed(order2)\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "# trainset is the same dataset for both but independently shuffled\n",
        "trainset1 = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset2 = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "trainloader1 = torch.utils.data.DataLoader(trainset1, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          generator=rand_shuffle_net1)\n",
        "\n",
        "trainloader2 = torch.utils.data.DataLoader(trainset2, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2,\n",
        "                                          generator=rand_shuffle_net2)\n",
        "\n",
        "# both networks have testloader\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# independent loss functions + optimizers\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer2 = optim.SGD(net2.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "# Train loop for net1\n",
        "print(\"Training Model 1\")\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader1, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer1.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net1(inputs)\n",
        "        loss = criterion1(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        # print statistics\n",
        "        if i % 2000 == 1999:\n",
        "            print(f\"[{epoch+1}, {i+1}] loss: {running_loss/2000:.3f}\")\n",
        "            running_loss = 0.0\n",
        "print(\"Finished training model 1\\n\")\n",
        "\n",
        "\n",
        "# train loop for net2\n",
        "print(\"Training Model 2\")\n",
        "\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader2, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer2.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net2(inputs)\n",
        "        loss = criterion2(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        # print statistics\n",
        "        if i % 2000 == 1999:\n",
        "            print(f\"[{epoch+1}, {i+1}] loss: {running_loss/2000:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished training model 2\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SDV0Tk5xNy6",
        "outputId": "593ec714-d8aa-460e-a49a-a9071aa13a45"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model 1\n",
            "[1, 2000] loss: 2.173\n",
            "[1, 4000] loss: 1.827\n",
            "[1, 6000] loss: 1.651\n",
            "[1, 8000] loss: 1.581\n",
            "[1, 10000] loss: 1.503\n",
            "[1, 12000] loss: 1.451\n",
            "[2, 2000] loss: 1.380\n",
            "[2, 4000] loss: 1.348\n",
            "[2, 6000] loss: 1.330\n",
            "[2, 8000] loss: 1.287\n",
            "[2, 10000] loss: 1.284\n",
            "[2, 12000] loss: 1.269\n",
            "[3, 2000] loss: 1.176\n",
            "[3, 4000] loss: 1.184\n",
            "[3, 6000] loss: 1.181\n",
            "[3, 8000] loss: 1.152\n",
            "[3, 10000] loss: 1.164\n",
            "[3, 12000] loss: 1.160\n",
            "[4, 2000] loss: 1.083\n",
            "[4, 4000] loss: 1.079\n",
            "[4, 6000] loss: 1.071\n",
            "[4, 8000] loss: 1.070\n",
            "[4, 10000] loss: 1.063\n",
            "[4, 12000] loss: 1.064\n",
            "[5, 2000] loss: 0.986\n",
            "[5, 4000] loss: 0.985\n",
            "[5, 6000] loss: 1.009\n",
            "[5, 8000] loss: 1.013\n",
            "[5, 10000] loss: 1.022\n",
            "[5, 12000] loss: 0.999\n",
            "Finished training model 1\n",
            "\n",
            "Training Model 2\n",
            "[1, 2000] loss: 2.127\n",
            "[1, 4000] loss: 1.785\n",
            "[1, 6000] loss: 1.625\n",
            "[1, 8000] loss: 1.555\n",
            "[1, 10000] loss: 1.512\n",
            "[1, 12000] loss: 1.458\n",
            "[2, 2000] loss: 1.409\n",
            "[2, 4000] loss: 1.363\n",
            "[2, 6000] loss: 1.338\n",
            "[2, 8000] loss: 1.323\n",
            "[2, 10000] loss: 1.311\n",
            "[2, 12000] loss: 1.282\n",
            "[3, 2000] loss: 1.210\n",
            "[3, 4000] loss: 1.229\n",
            "[3, 6000] loss: 1.194\n",
            "[3, 8000] loss: 1.215\n",
            "[3, 10000] loss: 1.193\n",
            "[3, 12000] loss: 1.192\n",
            "[4, 2000] loss: 1.125\n",
            "[4, 4000] loss: 1.122\n",
            "[4, 6000] loss: 1.106\n",
            "[4, 8000] loss: 1.094\n",
            "[4, 10000] loss: 1.127\n",
            "[4, 12000] loss: 1.114\n",
            "[5, 2000] loss: 1.039\n",
            "[5, 4000] loss: 1.038\n",
            "[5, 6000] loss: 1.049\n",
            "[5, 8000] loss: 1.042\n",
            "[5, 10000] loss: 1.038\n",
            "[5, 12000] loss: 1.065\n",
            "Finished training model 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get accuracy and agreement score"
      ],
      "metadata": {
        "id": "ekUmJ8Ai5lr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get test accuracy for both models\n",
        "def test_accuracy(net):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            # images, labels = data # had to change this line\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # calculate outputs by running images through the network\n",
        "            outputs = net(images)\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "# print results\n",
        "accuracy1 = test_accuracy(net1)\n",
        "accuracy2 = test_accuracy(net2)\n",
        "print(f'Accuracy of the net1 on the 10000 test images: {accuracy1:.2f} %')\n",
        "print(f'Accuracy of the net2 on the 10000 test images: {accuracy2:.2f} %')\n",
        "\n",
        "\n",
        "# calculate agreement score between both networks\n",
        "def ascore(net1, net2, test_images):\n",
        "\n",
        "    matches = 0 # prepare to count matches of predictions\n",
        "    total = 10000 # total amount of images in the dataset\n",
        "\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for images, _ in testloader:   # labels not needed as we just need to know which predictions matched between the 2 networks\n",
        "            images = images.to(device)\n",
        "\n",
        "            # top1 predictions from both networks\n",
        "            outputs1 = net1(images)\n",
        "            outputs2 = net2(images)\n",
        "\n",
        "            _, predicted1 = torch.max(outputs1, 1) # takes the highest energy from all 10 classes as its prediction\n",
        "            _, predicted2 = torch.max(outputs2, 1)\n",
        "\n",
        "            # count images where both predictions match\n",
        "            matches += (predicted1 == predicted2).sum().item()\n",
        "\n",
        "    return 100.0 * matches / total\n",
        "\n",
        "agreement_score = ascore(net1, net2, testloader)\n",
        "print(f\"Agreement Score between net1 and net2: {agreement_score:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvTZGGB2zcnH",
        "outputId": "d5a558de-29fd-46a7-a952-c82b7e8b60bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the net1 on the 10000 test images: 63.73 %\n",
            "Accuracy of the net2 on the 10000 test images: 59.60 %\n",
            "Agreement Score between net1 and net2: 60.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis\n",
        "\n",
        "In this task, I initialized and trained two different convolutional neural networks (CNNs), net1 and net2, on CIFAR-10 for five epochs each. The only differences between the two models were their random weight initializations and the random order in which they saw the training data. These small sources of randomness naturally lead each model down a different optimization path, which explains why their final accuracies are not identical. Net1 achieved a top-1 accuracy of 63.73% on the test set, while net2 reached 59.60%.\n",
        "\n",
        "When comparing the predictions of the two networks directly, I found that they agreed on 60.60% of the entire dataset. This agreement score reflects how often both models predicted the same class label, even if that label was incorrect. The value being close to the individual accuracies shows that the models are learning similar high-level representations but still diverge in meaningful ways. Since the networks start from different random weights and see the training samples in different orders, they naturally form slightly different decision boundaries, especially with the limited training time of only 5 epochs.\n",
        "\n",
        "The agreement score is no where close to 100% because the 2 networks are highly sensitive to initialization and data ordering, and some of the CIFAR-10 classes are somehwat similar in physical features which can be hard for the models to distinguish(e.g., cat vs. dog and truck vs. car). The stochastic differences lead the models to emphasize different features during training. As a result, they may correctly classify the same image or confidently disagree on others. Overall, the results match the expected behavior of independently trained neural networks and show how randomness affects the training and prediction outcomes in computer vision."
      ],
      "metadata": {
        "id": "TWXAcrIB5eMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "UFJ-6FJn5h36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get samples set for each class"
      ],
      "metadata": {
        "id": "o7UnUwQr9hbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get 100 test images from each class\n",
        "random.seed(0)  # make results reproducible\n",
        "\n",
        "# dictionary to store test image indices for each class\n",
        "indices_by_class = {a: [] for a in range(10)}\n",
        "\n",
        "# fill the dictionary with every indices belonging to each class\n",
        "for ind, (_, label) in enumerate(testset): # place each image in designated class\n",
        "    indices_by_class[label].append(ind)\n",
        "\n",
        "# sample 100 indices from each class\n",
        "sampled_indices_by_class = {}\n",
        "for a in range(10):\n",
        "    sampled_indices_by_class[a] = random.sample(indices_by_class[a], 100)\n"
      ],
      "metadata": {
        "id": "bPrSlICy5kiv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### calculate entropy for each class"
      ],
      "metadata": {
        "id": "UcQhYDAk9lQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get average entropy for each class\n",
        "def avg_entropy(net, testset, sampled_indices_by_class):\n",
        "\n",
        "    avg_entropy = {}\n",
        "    with torch.no_grad():\n",
        "        for a in range(10):\n",
        "            entropies = []\n",
        "\n",
        "            # fetch all sampled images for class c\n",
        "            indices = sampled_indices_by_class[a]\n",
        "\n",
        "            for index in indices:\n",
        "                image, _ = testset[index]\n",
        "                image = image.unsqueeze(0).to(device)  # add batch dimension because pytorch expects 4D input for CNN\n",
        "\n",
        "                # forward pass -> probabilities\n",
        "                logits = net(image) # network outputs raw scores not probabilities\n",
        "                probs = F.softmax(logits, dim=1) # use softmax to get probabilities and all values sum to 1\n",
        "\n",
        "                # entropy is the -sum(p * log(p))\n",
        "                entropy = -torch.sum(probs * torch.log(probs + 1e-12))\n",
        "                entropies.append(entropy.item()) # add entropy to list\n",
        "\n",
        "            # average entropy for this class\n",
        "            avg_entropy[a] = sum(entropies) / len(entropies)\n",
        "\n",
        "    return avg_entropy\n",
        "\n",
        "net1_entrop = avg_entropy(net1, testset, sampled_indices_by_class)\n",
        "\n",
        "print(\"Average entropy per class for net1:\\n\")\n",
        "for classname in range(10):\n",
        "    print(f\"Class {classes[classname]:5s}: {net1_entrop[classname]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu9lZDc9595m",
        "outputId": "a9555328-f6c8-45b0-d2c6-e1398aa6b107"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average entropy per class for net1:\n",
            "\n",
            "Class plane: 1.0838\n",
            "Class car  : 0.6380\n",
            "Class bird : 1.0738\n",
            "Class cat  : 1.2797\n",
            "Class deer : 1.3570\n",
            "Class dog  : 1.0907\n",
            "Class frog : 0.8397\n",
            "Class horse: 0.9125\n",
            "Class ship : 0.7504\n",
            "Class truck: 1.0301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### find accuracy for each class"
      ],
      "metadata": {
        "id": "KsJnELj5Q5Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def per_class_accuracy(net, testloader, classes):\n",
        "    # prepare to count predictions for each class\n",
        "    correct_pred = {classname: 0 for classname in classes}\n",
        "    total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader: # unpack data in for loop\n",
        "            # images, labels = data # can't load like this as\n",
        "            images, labels = images.to(device), labels.to(device) # assign o\n",
        "            outputs = net(images)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "\n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                label_name = classes[label]\n",
        "                # if label == prediction:\n",
        "                #     correct_pred[label_name] += 1\n",
        "                # total_pred[label_name] += 1\n",
        "                if label == prediction:\n",
        "                   correct_pred[classes[label]] += 1\n",
        "                total_pred[classes[label]] += 1\n",
        "\n",
        "    per_class_acc = {}\n",
        "    for classname in classes:\n",
        "        per_class_acc[classname] = 100.0 * float(correct_pred[classname]) / total_pred[classname]\n",
        "    return per_class_acc\n",
        "\n",
        "net1_class_acc = per_class_accuracy(net1, testloader, classes)\n",
        "\n",
        "# print accuracy for each class\n",
        "print(\"Per-class accuracy for net1:\\n\")\n",
        "for classname in classes:\n",
        "    print(f'Accuracy for class: {classname:5s} is {net1_class_acc[classname]:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdjOAyg76AO_",
        "outputId": "a138dfc5-de24-454e-c91a-511b460a7f4e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-class accuracy for net1:\n",
            "\n",
            "Accuracy for class: plane is 59.90%\n",
            "Accuracy for class: car   is 76.30%\n",
            "Accuracy for class: bird  is 52.90%\n",
            "Accuracy for class: cat   is 45.60%\n",
            "Accuracy for class: deer  is 51.60%\n",
            "Accuracy for class: dog   is 55.80%\n",
            "Accuracy for class: frog  is 78.10%\n",
            "Accuracy for class: horse is 67.50%\n",
            "Accuracy for class: ship  is 78.50%\n",
            "Accuracy for class: truck is 71.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis\n",
        "\n",
        "For the second task, I examined how confident the network is when making predictions by computing the entropy of its softmax output for 100 randomly sampled test images from each class. Low entropy indicates high confidence in the class prediction, while high entropy suggests uncertainty. Classes such as the car, frog, and ship had relatively low entropy scores (0.63, 0.84, and 0.75), meaning the network tended to produce confident probability distributions for these images. In contrast, classes like cat, deer, and dog showed much higher entropy (1.28, 1.36, and 1.09) which could indicate more uncertainty in their predictions.\n",
        "\n",
        "When aligned with the corresponding class accuracies, a noticeable pattern is present of classes with low entropy generally corresponding to higher accuracy, and classes with high entropy often show lower accuracy. For example, car, frog, ship, and truck all have strong accuracies above 70% while also being among the lowest-entropy classes. On the other hand, classes such as cat, deer, and bird show higher average entropy and also correspondingly lower accuracies, with some of the weakest performance in the dataset. This relationship makes sense because when the model is less certain (high entropy), it tends to make more mistakes, and when it is more confident (low entropy), it is generally correct.\n",
        "\n",
        "From this task, we see that the model's confidence is a useful indicator of its performance. Classes with visually distinct features, such as cars and ships, lead the model to make more confident and accurate predictions. Meanwhile, classes with higher variation or similarity when compared directly with another class, such as cats and dogs, causes the model to be more uncertain and inaccurate."
      ],
      "metadata": {
        "id": "vXpB_eClRvny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "AtOL-79PRyWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get new cifar100 images"
      ],
      "metadata": {
        "id": "x13lER_9oEFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get Cifar100 data set\n",
        "cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                              download=True, transform=transform)\n",
        "cifar100_classes = cifar100.classes"
      ],
      "metadata": {
        "id": "btPEe4kA6Ey_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### group images based on their classes"
      ],
      "metadata": {
        "id": "YDEh3ZWPoH6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make dictionary to store and group indices into their own classes\n",
        "indices_c100 = {c: [] for c in range(100)}\n",
        "\n",
        "for index, (_, label) in enumerate(cifar100):\n",
        "    indices_c100[label].append(index) # add right image label to the corresponding class\n"
      ],
      "metadata": {
        "id": "bLmzLq3KXQje"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get probability distribution of class"
      ],
      "metadata": {
        "id": "refu4xh-oOlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each class get the prediction distribution\n",
        "def pred_distrib(net, data, indices_by_class):\n",
        "    class_distributions = {}\n",
        "\n",
        "    # gradient not needed\n",
        "    with torch.no_grad():\n",
        "        for class_id in range(100): # go through all 100 classes for cifar100 dataset\n",
        "            counts = torch.zeros(10)  # cifar10 has 10 classes\n",
        "\n",
        "            for class_index in indices_by_class[class_id]:  # loop over 100 test images belonging to this class id\n",
        "                image, _ = data[class_index] # ignore label\n",
        "                image = image.unsqueeze(0).to(device) # add batch dimension because pytorch expects 4D input for CNN\n",
        "\n",
        "                logits = net(image)\n",
        "                _, pred = torch.max(logits, 1)  # cifar10 prediction\n",
        "\n",
        "                counts[pred.item()] += 1\n",
        "\n",
        "            class_distributions[class_id] = counts # store this per class result\n",
        "\n",
        "    return class_distributions\n",
        "\n",
        "c100_distribs = pred_distrib(net1, cifar100, indices_c100)\n",
        "\n",
        "# convert counts to probability distribution\n",
        "prob_distribs = {c: dist / dist.sum() for c, dist in c100_distribs.items()}"
      ],
      "metadata": {
        "id": "mt49nfRfXclX"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### find entropy of each class"
      ],
      "metadata": {
        "id": "mXcl06k7odzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# comput entropy for each class\n",
        "def prob_distr_entropy(p):\n",
        "    p = p + 1e-12\n",
        "    return float(-(p * torch.log(p)).sum())\n",
        "\n",
        "class_entropies = {} # create dict to store entropy for all 100 classes\n",
        "for class_id in range(100):\n",
        "    prob_vec = prob_distribs[class_id]   # 10-class probability distribution\n",
        "    class_entropies[class_id] = prob_distr_entropy(prob_vec)\n"
      ],
      "metadata": {
        "id": "k9Q7ZRimXohE"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### find best and worst classes for entropy"
      ],
      "metadata": {
        "id": "of1hOxNcoikc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert entropy dictionary to list of (class_index, entropy)\n",
        "entropy_list = list(class_entropies.items())\n",
        "\n",
        "# selection sort based on entropy (no lambda)\n",
        "for i in range(len(entropy_list)):\n",
        "    for j in range(i + 1, len(entropy_list)):\n",
        "        if entropy_list[j][1] < entropy_list[i][1]: # sort in ascending order by entropy\n",
        "            entropy_list[i], entropy_list[j] = entropy_list[j], entropy_list[i]\n",
        "\n",
        "# get lowest and highest 5 classes\n",
        "lowest_5  = entropy_list[:5]\n",
        "highest_5 = entropy_list[-5:]\n",
        "\n",
        "# print results\n",
        "print(\"\\nLowest 5 entropy classes:\")\n",
        "for classname, ent in lowest_5:\n",
        "    print(f\"Class {classname:3d} ({cifar100_classes[classname]}): {ent:.4f}\")\n",
        "\n",
        "print(\"\\nHighest 5 entropy classes:\")\n",
        "for classname, ent in highest_5:\n",
        "    print(f\"Class {classname:3d} ({cifar100_classes[classname]}): {ent:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVi-6AteX2Ob",
        "outputId": "d983ee76-1917-4ba5-f460-ea76d067b5d5"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lowest 5 entropy classes:\n",
            "Class  71 (sea): 1.0272\n",
            "Class  58 (pickup_truck): 1.0765\n",
            "Class   2 (baby): 1.3340\n",
            "Class  13 (bus): 1.3534\n",
            "Class  36 (hamster): 1.3587\n",
            "\n",
            "Highest 5 entropy classes:\n",
            "Class  91 (trout): 2.1523\n",
            "Class  55 (otter): 2.1553\n",
            "Class  99 (worm): 2.1610\n",
            "Class  84 (table): 2.1752\n",
            "Class  75 (skunk): 2.2401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gaussian Noise"
      ],
      "metadata": {
        "id": "zpjf4Tq0oodQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create gaussian noise\n",
        "# create 100 Gaussian noise images (mean=0, std=1) woth a shape: [batch_size=100, channels=3, height=32, width=32]\n",
        "noise_images = torch.randn(100, 3, 32, 32).to(device) # torch.randn() creates random numbers from a normal distribution with mean = 0 and std deviation = 0\n",
        "\n",
        "# predict classes given noise\n",
        "with torch.no_grad():\n",
        "    logits = net1(noise_images) # outputs for all 100 noise images\n",
        "    preds = logits.argmax(dim=1) # top1 class predictions\n",
        "\n",
        "# count predictions\n",
        "noise_counts = torch.bincount(preds, minlength=10)\n",
        "noise_prob = noise_counts / noise_counts.sum()\n",
        "\n",
        "# compute entropy of the noise distribution\n",
        "noise_entropy = prob_distr_entropy(noise_prob)\n",
        "\n",
        "# print results\n",
        "print(\"Noise prediction distribution:\\n\", noise_prob)\n",
        "print(\"\\nNoise entropy:\", noise_entropy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QYaLZctYJBS",
        "outputId": "f5c382fd-f972-4bbd-eeec-2d3281e34899"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise prediction distribution:\n",
            " tensor([0.0000, 0.0700, 0.0000, 0.0100, 0.0000, 0.0000, 0.0400, 0.0500, 0.0000,\n",
            "        0.8300], device='cuda:0')\n",
            "\n",
            "Noise entropy: 0.6653951406478882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis\n",
        "\n",
        "For each CIFAR-100 class, I collected all 100 predictions made by the network and transformed them into a probability distribution over the ten CIFAR-10 labels. The lowest-entropy classes, such as sea, pickup_truck, and bus, produced concentrated prediction distributions, meaning the model consistently mapped most images from these categories into a single CIFAR-10 class. In contrast, the highest-entropy classes like trout, otter, worm, and skunk, generated very scattered predictions, suggesting the network had difficulty associating these image types with any single CIFAR-10 category.\n",
        "\n",
        "These entropy patterns reflect how visually related each CIFAR-100 class is to the CIFAR-10 classes the network was trained on. Classes with low entropy often resemble or share features with a specific CIFAR-10 category. For example, the pickup_truck is visually similar to CIFAR-10’s “truck”, and sea often contains textures or colors that the model might consistently map to classes like “ship.” On the other hand, classes like trout, otter, and worm do not strongly resemble any one CIFAR-10 class, leading to a higher entropy because the network spreads its predictions across many categories. This behavior makes sense given that CIFAR-10 contains only broad animal categories (bird, cat, dog, frog), while many CIFAR-100 classes represent more specific species or objects (bus is a type of truck) with features that do not exactly match up with the CIFAR-10 classes.\n",
        "\n",
        "Finally, when the network was fed 100 samples of pure Gaussian noise, the predictions collapsed heavily into a few classes. Most notably the truck class, accounted for 83% of predictions. The resulting entropy was relatively low (0.665), showing that the model was surprisingly confident even though the inputs contained no meaningful structure. Instead of recognizing that noise is nonsensical, the network was forced to map every input to the nearest learned pattern and it did this with high confidence."
      ],
      "metadata": {
        "id": "U2Y1-kCRoq0A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mTvtg8bFZGst"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "YDsocSbsxEnW",
        "s_IbAxcN50XU",
        "UFJ-6FJn5h36",
        "o7UnUwQr9hbn",
        "AtOL-79PRyWY",
        "x13lER_9oEFm",
        "YDEh3ZWPoH6E",
        "refu4xh-oOlf"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}